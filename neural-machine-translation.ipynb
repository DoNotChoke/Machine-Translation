{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:45:31.451158Z","iopub.execute_input":"2025-03-04T13:45:31.451447Z","iopub.status.idle":"2025-03-04T13:45:35.848540Z","shell.execute_reply.started":"2025-03-04T13:45:31.451426Z","shell.execute_reply":"2025-03-04T13:45:35.847633Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"thainq107/iwslt2015-en-vi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:45:35.849715Z","iopub.execute_input":"2025-03-04T13:45:35.849953Z","iopub.status.idle":"2025-03-04T13:45:40.023151Z","shell.execute_reply.started":"2025-03-04T13:45:35.849932Z","shell.execute_reply":"2025-03-04T13:45:40.022049Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/522 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d79082c4f75f4ec787ab53c8e13a7b3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/17.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46de043068e64025bb4d72e3e2b31115"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/181k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bd48df8ffe148b09997d2a2b6b5d794"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/133317 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b011fab39a046cfacd9cb07e4edcc1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3da570bad3864515813e9f467199f471"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbe291a0d5094af7abba974eac095ecd"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:45:40.024664Z","iopub.execute_input":"2025-03-04T13:45:40.025164Z","iopub.status.idle":"2025-03-04T13:45:40.031353Z","shell.execute_reply.started":"2025-03-04T13:45:40.025136Z","shell.execute_reply":"2025-03-04T13:45:40.030273Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['en', 'vi'],\n        num_rows: 133317\n    })\n    validation: Dataset({\n        features: ['en', 'vi'],\n        num_rows: 1268\n    })\n    test: Dataset({\n        features: ['en', 'vi'],\n        num_rows: 1268\n    })\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"from tokenizers import Tokenizer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.models import WordLevel\n\ntokenizer_en = Tokenizer(WordLevel(unk_token=\"<unk>\"))\ntokenizer_vi = Tokenizer(WordLevel(unk_token=\"<unk>\"))\n\ntokenizer_en.pre_tokenizer = Whitespace()\ntokenizer_vi.pre_tokenizer = Whitespace()\n\ntrainer = WordLevelTrainer(\n    vocab_size=15000,\n    min_frequency=2,\n    special_tokens=[\"<pad>\",\"<unk>\",\"<bos>\", \"<eos>\"]\n)\n\ntokenizer_en.train_from_iterator(ds[\"train\"][\"en\"], trainer)\ntokenizer_vi.train_from_iterator(ds[\"train\"][\"vi\"], trainer)\n\ntokenizer_en.save(\"tokenizer_en.json\")\ntokenizer_vi.save(\"tokenizer_vi.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:45:47.306773Z","iopub.execute_input":"2025-03-04T13:45:47.307160Z","iopub.status.idle":"2025-03-04T13:45:50.610723Z","shell.execute_reply.started":"2025-03-04T13:45:47.307130Z","shell.execute_reply":"2025-03-04T13:45:50.609793Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast\n\ntokenizer_en = PreTrainedTokenizerFast(\n    tokenizer_file=\"tokenizer_en.json\",\n    unk_token=\"<unk>\", pad_token=\"<pad>\", bos_token=\"<bos>\", eos_token=\"<eos>\"\n)\n\ntokenizer_vi = PreTrainedTokenizerFast(\n    tokenizer_file=\"tokenizer_vi.json\",\n    unk_token=\"<unk>\", pad_token=\"<pad>\", bos_token=\"<bos>\", eos_token=\"<eos>\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:45:52.556348Z","iopub.execute_input":"2025-03-04T13:45:52.556693Z","iopub.status.idle":"2025-03-04T13:45:56.198949Z","shell.execute_reply.started":"2025-03-04T13:45:52.556664Z","shell.execute_reply":"2025-03-04T13:45:56.198048Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9822b07b5cf44a6e8b1e86a3e9b5eda4"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"MAX_LEN = 75\n\ndef preprocess_function(examples):\n    src_texts = examples[\"en\"]\n    tgt_texts = [\"<bos>\" + text + \"<eos>\" for text in examples[\"vi\"]]\n\n    src_encodings = tokenizer_en(\n        src_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN\n    )\n    tgt_encodings = tokenizer_vi(\n        tgt_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN\n    )\n    return {\n        \"input_ids\" : src_encodings[\"input_ids\"],\n        \"labels\" : tgt_encodings[\"input_ids\"]\n    }\n\npreprocessed_ds = ds.map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:46:01.081317Z","iopub.execute_input":"2025-03-04T13:46:01.081840Z","iopub.status.idle":"2025-03-04T13:46:15.212726Z","shell.execute_reply.started":"2025-03-04T13:46:01.081811Z","shell.execute_reply":"2025-03-04T13:46:15.212043Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/133317 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d1587d5587b4258b10af414640936f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71c37f59187d4925b48eaa44f0daa0b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18dd59343ed74e868ddd9da0ae9e400a"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import PreTrainedModel, PretrainedConfig\n\nclass Seq2SeqRNNConfig(PretrainedConfig):\n    def __init__(self, vocab_size_src=15000, vocab_size_tgt=15000, embed_dim=128, hidden_size=128, dropout=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.vocab_size_src = vocab_size_src\n        self.vocab_size_tgt = vocab_size_tgt\n        self.embed_dim = embed_dim\n        self.hidden_size = hidden_size\n        self.dropout= dropout\n\nclass RNNEncoder(nn.Module):\n    def __init__(self, input_size, embed_dim, hidden_size, dropout=0.1):\n        super(RNNEncoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(input_size, embed_dim)\n        self.gru = nn.GRU(embed_dim, hidden_size, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input):\n        embedded = self.dropout(self.embedding(input))\n        output, hidden = self.gru(embedded)\n        return output, hidden\n\nclass RNNDecoder(nn.Module):\n    def __init__(self, hidden_size, embed_dim, output_size):\n        super(RNNDecoder, self).__init__()\n        self.embedding = nn.Embedding(output_size, embed_dim)\n        self.gru = nn.GRU(embed_dim, hidden_size, batch_first=True)\n        self.out = nn.Linear(hidden_size, output_size)\n    def forward(self, input, hidden):\n        output = self.embedding(input)\n        output, hidden = self.gru(output, hidden)\n        output = self.out(output)\n        return output, hidden\n\nclass Seq2SeqRNNModel(PreTrainedModel):\n    config_class = Seq2SeqRNNConfig\n\n    def __init__(self, config, tokenizer_en):\n        super(Seq2SeqRNNModel, self).__init__(config)\n        self.encoder = RNNEncoder(\n            config.vocab_size_src,\n            config.hidden_size,\n            config.hidden_size,\n            config.dropout\n        )\n        self.decoder = RNNDecoder(\n            config.hidden_size,\n            config.embed_dim,\n            config.vocab_size_tgt\n        )\n        self.BOS_IDX = tokenizer_en.bos_token_id\n        self.loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n\n    def forward(self, input_ids, labels):\n        batch_size, seq_len = labels.shape\n        decoder_input = torch.full((batch_size, 1), self.BOS_IDX, dtype=torch.long).to(input_ids.device)\n        encoder_output, decoder_hidden = self.encoder(input_ids)\n        decoder_outputs = []\n\n        for i in range(seq_len):\n            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n            decoder_outputs.append(decoder_output)\n            decoder_input = labels[:, i].unsqueeze(1)\n\n        logits = torch.cat(decoder_outputs, dim=1)\n        loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n        return {\"loss\": loss, \"logits\": logits}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T14:40:40.281908Z","iopub.execute_input":"2025-03-04T14:40:40.282335Z","iopub.status.idle":"2025-03-04T14:40:40.296264Z","shell.execute_reply.started":"2025-03-04T14:40:40.282302Z","shell.execute_reply":"2025-03-04T14:40:40.295268Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"config = Seq2SeqRNNConfig(\n    vocab_size_src=len(tokenizer_en), vocab_size_tgt=len(tokenizer_vi)\n)\nmodel = Seq2SeqRNNModel(config, tokenizer_en)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T14:40:42.259700Z","iopub.execute_input":"2025-03-04T14:40:42.260004Z","iopub.status.idle":"2025-03-04T14:40:42.316389Z","shell.execute_reply.started":"2025-03-04T14:40:42.259961Z","shell.execute_reply":"2025-03-04T14:40:42.315611Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"Seq2SeqRNNModel(\n  (encoder): RNNEncoder(\n    (embedding): Embedding(15000, 128)\n    (gru): GRU(128, 128, batch_first=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): RNNDecoder(\n    (embedding): Embedding(13684, 128)\n    (gru): GRU(128, 128, batch_first=True)\n    (out): Linear(in_features=128, out_features=13684, bias=True)\n  )\n  (loss_fn): CrossEntropyLoss()\n)"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"mask = (torch.triu(torch.ones((5, 5))) == 1).transpose(0, 1)\nmask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:09:32.195312Z","iopub.execute_input":"2025-03-04T15:09:32.195621Z","iopub.status.idle":"2025-03-04T15:09:32.207430Z","shell.execute_reply.started":"2025-03-04T15:09:32.195598Z","shell.execute_reply":"2025-03-04T15:09:32.206358Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"tensor([[ True, False, False, False, False],\n        [ True,  True, False, False, False],\n        [ True,  True,  True, False, False],\n        [ True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True]])"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import PreTrainedModel, PretrainedConfig\n\ndef generate_square_subsequent_mask(sz, device):\n    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\ndef create_mask(src, tgt):\n    src_seq_len = src.shape[1]\n    tgt_seq_len = tgt.shape[1]\n    device = src.device\n\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len, device).to(torch.bool)\n    src_mask = torch.zeros((src_seq_len, src_seq_len),device=device).type(torch.bool)\n    src_padding_mask = (src == 0)\n    tgt_padding_mask = (tgt == 0)\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n\nclass Seq2SeqTransformerConfig(PretrainedConfig):\n    def __init__(\n            self, vocab_size_src=10000, vocab_size_tgt=10000, max_seq_length=50,\n            d_model=256, num_heads=8, num_layers=6, dropout=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.vocab_size_src = vocab_size_src\n        self.vocab_size_tgt = vocab_size_tgt\n        self.max_seq_length = max_seq_length\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.num_layers = num_layers\n        self.dropout = dropout\n\nclass Seq2SeqTransformerModel(PreTrainedModel):\n    config_class = Seq2SeqTransformerConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.embedding_src = nn.Embedding(\n            config.vocab_size_src, config.d_model)\n        self.embedding_tgt = nn.Embedding(\n            config.vocab_size_tgt, config.d_model)\n\n        self.position_embedding_src = nn.Embedding(\n            config.max_seq_length, config.d_model)\n        self.position_embedding_tgt = nn.Embedding(\n            config.max_seq_length, config.d_model)\n\n        self.transformer = nn.Transformer(\n            d_model=config.d_model,\n            nhead=config.num_heads,\n            num_encoder_layers=config.num_layers,\n            num_decoder_layers=config.num_layers,\n            dropout=config.dropout,\n            batch_first=True\n        )\n\n        self.generator = nn.Linear(\n            config.d_model, config.vocab_size_tgt\n            )\n        self.loss_fn = nn.CrossEntropyLoss(ignore_index=0)  # Ignore PAD token\n\n    def forward(self, input_ids, labels):\n        tgt_input = labels[:, :-1]\n        tgt_output = labels[:, 1:]\n        batch_size, seq_len_src = input_ids.shape\n        _, seq_len_tgt = tgt_input.shape\n\n        src_positions = torch.arange(seq_len_src, device=input_ids.device).unsqueeze(0)\n        tgt_positions = torch.arange(seq_len_tgt, device=labels.device).unsqueeze(0)\n\n        src_embedded = self.embedding_src(input_ids) + self.position_embedding_src(src_positions)\n        tgt_embedded = self.embedding_tgt(tgt_input) + self.position_embedding_tgt(tgt_positions)\n\n        src_mask, tgt_mask, src_key_padding_mask, tgt_key_padding_mask = create_mask(input_ids, tgt_input)\n\n        outs = self.transformer(\n            src_embedded, tgt_embedded, src_mask, tgt_mask,\n            src_key_padding_mask=src_key_padding_mask,\n            tgt_key_padding_mask=tgt_key_padding_mask\n        )\n\n        logits = self.generator(outs)\n        loss = self.loss_fn(logits.permute(0, 2, 1), tgt_output)\n\n        return {\"loss\": loss, \"logits\": logits}\n\n    def encode(self, src, src_mask):\n        _, seq_len_src = src.shape\n        src_positions = torch.arange(\n            seq_len_src, device=src.device).unsqueeze(0)\n        src_embedded = self.embedding_src(src) + self.position_embedding_src(src_positions)\n        return self.transformer.encoder(src_embedded, src_mask)\n\n    def decode(self, tgt, encoder_output, tgt_mask):\n        _, seq_len_tgt = tgt.shape\n        tgt_positions = torch.arange(seq_len_tgt, device=tgt.device).unsqueeze(0)\n        tgt_embedded = self.embedding_tgt(tgt) + self.position_embedding_tgt(tgt_positions)\n        return self.transformer.decoder(\n            tgt_embedded, encoder_output, tgt_mask\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:33:33.477202Z","iopub.execute_input":"2025-03-04T15:33:33.477620Z","iopub.status.idle":"2025-03-04T15:33:33.491130Z","shell.execute_reply.started":"2025-03-04T15:33:33.477589Z","shell.execute_reply":"2025-03-04T15:33:33.490297Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Khởi tạo config\nconfig = Seq2SeqTransformerConfig(\n    vocab_size_src=len(tokenizer_en), vocab_size_tgt=len(tokenizer_vi), max_seq_length=75\n)\n\n# Tạo mô hình\nmodel = Seq2SeqTransformerModel(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:33:37.808627Z","iopub.execute_input":"2025-03-04T15:33:37.808905Z","iopub.status.idle":"2025-03-04T15:33:38.068350Z","shell.execute_reply.started":"2025-03-04T15:33:37.808885Z","shell.execute_reply":"2025-03-04T15:33:38.067419Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def greedy_search(model, src, src_mask, max_len, start_symbol, device=\"cpu\"):\n    src = src.to(device)\n    src_mask = src_mask.to(device)\n\n    memory = model.encode(src, src_mask)\n    ys = torch.ones(1,1).fill_(start_symbol).type(torch.long).to(device)\n\n    for i in range(max_len-1):\n        memory = memory.to(device)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(1), deivice).type(torch.bool)).to(device)\n        out = model.decode(ys, memory, tgt_mask)\n        prob = model.generator(out[:, -1, :])\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word[-1].item()\n        ys = torch.cat([ys,torch.ones(1, 1).type_as(\n            src.data).fill_(next_word)], dim=1)\n        if next_word == 3: #\"<eos>\"\n            break\n    return ys","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:56:20.674044Z","iopub.execute_input":"2025-03-04T15:56:20.674419Z","iopub.status.idle":"2025-03-04T15:56:20.680550Z","shell.execute_reply.started":"2025-03-04T15:56:20.674395Z","shell.execute_reply":"2025-03-04T15:56:20.679564Z"}},"outputs":[],"execution_count":28}]}